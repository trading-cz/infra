name: Deploy K3s Cluster

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy'
        required: true
        type: choice
        options:
          - dev
          - prod

env:
  TF_VERSION: '1.13.4'

jobs:
  deploy:
    name: Deploy ${{ inputs.environment }} cluster
    runs-on: ubuntu-latest
    
    steps:
      # ============================================
      # Setup
      # ============================================
      - name: Checkout Infra Repository
        uses: actions/checkout@v4
        with:
          path: infra

      - name: Checkout Config Repository
        uses: actions/checkout@v4
        with:
          repository: trading-cz/config
          path: config
          token: ${{ secrets.TOKEN_GIT_REPO_CONFIG }}

      - name: Setup Terraform ${{ env.TF_VERSION }}
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: Install hcloud CLI
        run: |
          curl -fsSL https://github.com/hetznercloud/cli/releases/download/v1.47.0/hcloud-linux-amd64.tar.gz | tar -xz
          chmod +x hcloud
          sudo mv hcloud /usr/local/bin/
          hcloud version

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          kubectl version --client

      - name: Setup SSH keys
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_ed25519
          echo "${{ secrets.SSH_PUBLIC_KEY }}" > ~/.ssh/id_ed25519.pub
          chmod 600 ~/.ssh/id_ed25519
          chmod 644 ~/.ssh/id_ed25519.pub
          ssh-keygen -y -f ~/.ssh/id_ed25519 > ~/.ssh/id_ed25519.pub.verify
          echo "‚úÖ SSH keys configured"

      # ============================================
      # Primary IPs - Create/Reuse Before Terraform
      # ============================================
      - name: Create or Get Primary IPs
        id: primary_ips
        run: |
          export HCLOUD_TOKEN="${{ secrets.HCLOUD_TOKEN }}"
          
          ENV="${{ inputs.environment }}"
          CLUSTER="k3s-trading"
          
          echo "üîç Checking for existing Primary IPs..."
          
          # Control Plane Primary IP
          CONTROL_IP_NAME="${ENV}-${CLUSTER}-control-ip"
          CONTROL_IP_ID=$(hcloud primary-ip list -o noheader -o columns=id,name | grep "$CONTROL_IP_NAME" | awk '{print $1}')
          
          if [ -z "$CONTROL_IP_ID" ]; then
            echo "üìå Creating Primary IP: $CONTROL_IP_NAME"
            CONTROL_IP_ID=$(hcloud primary-ip create --type ipv4 --name "$CONTROL_IP_NAME" --datacenter nbg1-dc3 --label environment=$ENV --label cluster=$CLUSTER --label role=control-plane -o json | jq -r '.primary_ip.id')
            echo "‚úÖ Created Primary IP ID: $CONTROL_IP_ID"
          else
            echo "‚úÖ Found existing Primary IP: $CONTROL_IP_NAME (ID: $CONTROL_IP_ID)"
          fi
          echo "control_plane_primary_ip_id=$CONTROL_IP_ID" >> $GITHUB_OUTPUT
          
          # Kafka Primary IP
          KAFKA_IP_NAME="${ENV}-${CLUSTER}-kafka-ip"
          KAFKA_IP_ID=$(hcloud primary-ip list -o noheader -o columns=id,name | grep "$KAFKA_IP_NAME" | awk '{print $1}')
          
          if [ -z "$KAFKA_IP_ID" ]; then
            echo "üìå Creating Primary IP: $KAFKA_IP_NAME"
            KAFKA_IP_ID=$(hcloud primary-ip create --type ipv4 --name "$KAFKA_IP_NAME" --datacenter nbg1-dc3 --label environment=$ENV --label cluster=$CLUSTER --label role=kafka -o json | jq -r '.primary_ip.id')
            echo "‚úÖ Created Primary IP ID: $KAFKA_IP_ID"
          else
            echo "‚úÖ Found existing Primary IP: $KAFKA_IP_NAME (ID: $KAFKA_IP_ID)"
          fi
          echo "kafka_primary_ip_id=$KAFKA_IP_ID" >> $GITHUB_OUTPUT
          
          echo ""
          echo "üìã Primary IP Summary:"
          hcloud primary-ip list

      # ============================================
      # Terraform - Infrastructure Provisioning
      # ============================================
      - name: Terraform Init
        working-directory: infra
        run: terraform init

      - name: Terraform Plan
        working-directory: infra
        run: terraform plan -var-file="environments/${{ inputs.environment }}.tfvars" -out=tfplan
        env:
          TF_VAR_hcloud_token: ${{ secrets.HCLOUD_TOKEN }}
          TF_VAR_ssh_public_key: ${{ secrets.SSH_PUBLIC_KEY }}
          TF_VAR_control_plane_primary_ip_id: ${{ steps.primary_ips.outputs.control_plane_primary_ip_id }}
          TF_VAR_kafka_primary_ip_id: ${{ steps.primary_ips.outputs.kafka_primary_ip_id }}

      - name: Terraform Apply
        working-directory: infra
        run: terraform apply -auto-approve tfplan
        env:
          TF_VAR_hcloud_token: ${{ secrets.HCLOUD_TOKEN }}
          TF_VAR_ssh_public_key: ${{ secrets.SSH_PUBLIC_KEY }}
          TF_VAR_control_plane_primary_ip_id: ${{ steps.primary_ips.outputs.control_plane_primary_ip_id }}
          TF_VAR_kafka_primary_ip_id: ${{ steps.primary_ips.outputs.kafka_primary_ip_id }}

      - name: Extract Terraform Outputs
        id: tf_output
        working-directory: infra
        run: |
          CONTROL_IP=$(terraform output -raw k3s_control_public_ip)
          KAFKA_IP=$(terraform output -json kafka_server_public_ips | jq -r '.[0]')
          echo "control_ip=$CONTROL_IP" >> $GITHUB_OUTPUT
          echo "kafka_ip=$KAFKA_IP" >> $GITHUB_OUTPUT
          echo "‚úÖ Control Plane IP: $CONTROL_IP"
          echo "‚úÖ Kafka-0 Node IP: $KAFKA_IP"

      # ============================================
      # Verify 1: Servers are reachable
      # ============================================
      - name: Verify - Servers are accessible via SSH
        run: |
          CONTROL_IP=${{ steps.tf_output.outputs.control_ip }}
          KAFKA_IP=${{ steps.tf_output.outputs.kafka_ip }}
          
          echo "================================================"
          echo "Waiting for SSH to be available..."
          echo "Control Plane IP: $CONTROL_IP"
          echo "Kafka-0 Node IP:  $KAFKA_IP"
          echo "================================================"
          
          # Wait for control plane SSH
          for i in {1..30}; do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@$CONTROL_IP "echo 'SSH ready'" 2>/dev/null; then
              echo "‚úÖ Control plane SSH accessible at $CONTROL_IP"
              break
            fi
            echo "Attempt $i/30: Waiting for control plane SSH at $CONTROL_IP..."
            sleep 10
          done
          
          # Wait for Kafka node SSH
          for i in {1..30}; do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@$KAFKA_IP "echo 'SSH ready'" 2>/dev/null; then
              echo "‚úÖ Kafka node SSH accessible at $KAFKA_IP"
              break
            fi
            echo "Attempt $i/30: Waiting for Kafka node SSH at $KAFKA_IP..."
            sleep 10
          done

      # ============================================
      # Verify 2: Cloud-init completed
      # ============================================
      - name: Verify - Cloud-init installation completed
        run: |
          CONTROL_IP=${{ steps.tf_output.outputs.control_ip }}
          KAFKA_IP=${{ steps.tf_output.outputs.kafka_ip }}
          
          echo "Waiting for cloud-init to complete..."
          
          # Wait for control plane cloud-init
          CONTROL_READY=false
          for i in {1..60}; do
            if ssh -o StrictHostKeyChecking=no root@$CONTROL_IP "test -f /root/k3s-ready.txt" 2>/dev/null; then
              echo "‚úÖ Control plane cloud-init completed"
              CONTROL_READY=true
              break
            fi
            echo "Attempt $i/60: Waiting for control plane cloud-init (checking /root/k3s-ready.txt)..."
            sleep 5
          done
          
          # If control plane not ready, show logs
          if [ "$CONTROL_READY" = "false" ]; then
            echo "‚ùå Control plane cloud-init failed or timed out"
            echo "==== Cloud-init user data ===="
            ssh -o StrictHostKeyChecking=no root@$CONTROL_IP "cat /var/lib/cloud/instance/user-data.txt" 2>/dev/null || echo "No user-data.txt found"
            echo "==== Cloud-init log ===="
            ssh -o StrictHostKeyChecking=no root@$CONTROL_IP "cat /root/cloud-init.log" 2>/dev/null || echo "No cloud-init.log found"
            echo "==== Cloud-init output log ===="
            ssh -o StrictHostKeyChecking=no root@$CONTROL_IP "cat /var/log/cloud-init-output.log | tail -100" 2>/dev/null || echo "No cloud-init-output.log found"
            echo "==== Cloud-init status ===="
            ssh -o StrictHostKeyChecking=no root@$CONTROL_IP "cloud-init status --long" 2>/dev/null || echo "cloud-init status unavailable"
            echo "==== System journal (last 50 lines) ===="
            ssh -o StrictHostKeyChecking=no root@$CONTROL_IP "journalctl -u cloud-final.service -n 50 --no-pager" 2>/dev/null || echo "Journal unavailable"
            exit 1
          fi
          
          # Retrieve K3s token from control plane and push to worker nodes
          echo "üîë Retrieving K3s node token from control plane..."
          K3S_TOKEN=$(ssh -o StrictHostKeyChecking=no root@$CONTROL_IP "cat /var/lib/rancher/k3s/server/node-token" 2>/dev/null)
          if [ -n "$K3S_TOKEN" ]; then
            echo "‚úÖ K3s token retrieved successfully"
            echo "üì§ Pushing token to Kafka node..."
            ssh -o StrictHostKeyChecking=no root@$KAFKA_IP "echo '$K3S_TOKEN' > /tmp/k3s-token && chmod 600 /tmp/k3s-token"
            echo "‚úÖ Token pushed to worker node"
          else
            echo "‚ùå Failed to retrieve K3s token from control plane"
            exit 1
          fi
          
          # Wait for Kafka node cloud-init
          KAFKA_READY=false
          for i in {1..60}; do
            if ssh -o StrictHostKeyChecking=no root@$KAFKA_IP "test -f /root/k3s-agent-ready.txt" 2>/dev/null; then
              echo "‚úÖ Kafka node cloud-init completed"
              KAFKA_READY=true
              break
            fi
            echo "Attempt $i/60: Waiting for Kafka node cloud-init (checking /root/k3s-agent-ready.txt)..."
            sleep 5
          done
          
          # If Kafka node not ready, show logs
          if [ "$KAFKA_READY" = "false" ]; then
            echo "‚ùå Kafka node cloud-init failed or timed out"
            echo "==== Cloud-init log ===="
            ssh -o StrictHostKeyChecking=no root@$KAFKA_IP "cat /root/cloud-init.log" 2>/dev/null || echo "No cloud-init.log found"
            echo "==== Cloud-init status ===="
            ssh -o StrictHostKeyChecking=no root@$KAFKA_IP "cloud-init status" 2>/dev/null || echo "cloud-init status unavailable"
            echo "==== System journal (last 50 lines) ===="
            ssh -o StrictHostKeyChecking=no root@$KAFKA_IP "journalctl -u cloud-final.service -n 50 --no-pager" 2>/dev/null || echo "Journal unavailable"
            exit 1
          fi

      # ============================================
      # Verify 3: K3s cluster is operational
      # ============================================
      - name: Verify - K3s cluster is running
        run: |
          CONTROL_IP=${{ steps.tf_output.outputs.control_ip }}
          
          echo "Fetching kubeconfig..."
          ssh -o StrictHostKeyChecking=no root@$CONTROL_IP "cat /etc/rancher/k3s/k3s.yaml" > kubeconfig.yaml
          sed -i "s/127.0.0.1/$CONTROL_IP/g" kubeconfig.yaml
          export KUBECONFIG=$PWD/kubeconfig.yaml
          
          echo "Verifying K3s cluster..."
          kubectl get nodes
          
          NODE_COUNT=$(kubectl get nodes --no-headers | wc -l)
          if [ "$NODE_COUNT" -ge 2 ]; then
            echo "‚úÖ K3s cluster operational with $NODE_COUNT nodes"
          else
            echo "‚ùå Expected at least 2 nodes, found $NODE_COUNT"
            exit 1
          fi

      # ============================================
      # Verify 4: System pods are running
      # ============================================
      - name: Verify - K3s system pods are running
        run: |
          CONTROL_IP=${{ steps.tf_output.outputs.control_ip }}
          export KUBECONFIG=$PWD/kubeconfig.yaml
          
          echo "Checking system pods..."
          kubectl get pods -A
          
          echo ""
          echo "Waiting for CoreDNS to be ready..."
          kubectl wait --for=condition=ready pod -l k8s-app=kube-dns -n kube-system --timeout=300s
          echo "‚úÖ CoreDNS is ready"
          
          echo ""
          echo "Checking for critical system pods..."
          COREDNS_COUNT=$(kubectl get pods -n kube-system -l k8s-app=kube-dns --no-headers | grep Running | wc -l)
          if [ "$COREDNS_COUNT" -ge 1 ]; then
            echo "‚úÖ CoreDNS running ($COREDNS_COUNT pods)"
          else
            echo "‚ùå CoreDNS not running"
            exit 1
          fi

      # ============================================
      # Verify 5: ArgoCD is installed
      # ============================================
      - name: Verify - ArgoCD is installed and running
        run: |
          export KUBECONFIG=$PWD/kubeconfig.yaml
          
          echo "Checking ArgoCD namespace..."
          if kubectl get namespace argocd 2>/dev/null; then
            echo "‚úÖ ArgoCD namespace exists"
          else
            echo "‚ùå ArgoCD namespace not found"
            exit 1
          fi
          
          echo ""
          echo "Waiting for ArgoCD pods to be ready..."
          # Wait for pods to exist first
          for i in {1..60}; do
            POD_COUNT=$(kubectl get pods -n argocd --no-headers 2>/dev/null | wc -l)
            if [ "$POD_COUNT" -ge 3 ]; then
              echo "Found $POD_COUNT ArgoCD pods, waiting for ready state..."
              break
            fi
            echo "Waiting for ArgoCD pods to be created... ($i/60)"
            sleep 2
          done
          
          # Check if pods were created
          POD_COUNT=$(kubectl get pods -n argocd --no-headers 2>/dev/null | wc -l)
          if [ "$POD_COUNT" -lt 3 ]; then
            echo "‚ùå ArgoCD pods not created after 2 minutes"
            kubectl get pods -n argocd
            exit 1
          fi
          
          # Now wait for pods to be ready (5 minute timeout)
          # Use --all to wait for all pods in the namespace instead of label selector
          if ! kubectl wait --for=condition=ready pod --all -n argocd --timeout=300s; then
            echo "‚ùå ArgoCD pods not ready after 5 minutes"
            echo ""
            echo "Current pod status:"
            kubectl get pods -n argocd
            echo ""
            echo "Pod details:"
            kubectl describe pods -n argocd
            exit 1
          fi
          
          echo ""
          echo "ArgoCD pod status:"
          kubectl get pods -n argocd
          
          ARGOCD_READY=$(kubectl get pods -n argocd --no-headers | grep Running | wc -l)
          echo "‚úÖ ArgoCD is running ($ARGOCD_READY pods ready)"
          
          echo ""
          echo "Checking ArgoCD services..."
          kubectl get svc -n argocd
          
          # Verify critical services exist
          if kubectl get svc argocd-server -n argocd > /dev/null 2>&1; then
            echo "‚úÖ ArgoCD server service exists"
          else
            echo "‚ùå ArgoCD server service not found"
            exit 1
          fi

      # ============================================
      # Verify 6: Traefik ingress is running
      # ============================================
      - name: Verify - Traefik ingress controller
        run: |
          export KUBECONFIG=$PWD/kubeconfig.yaml
          
          echo "Checking Traefik (K3s built-in)..."
          
          # Check for Traefik in kube-system namespace (K3s default)
          if kubectl get deployment traefik -n kube-system 2>/dev/null; then
            echo "‚úÖ Traefik deployment found in kube-system"
            
            TRAEFIK_READY=$(kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik --no-headers | grep Running | wc -l)
            if [ "$TRAEFIK_READY" -ge 1 ]; then
              echo "‚úÖ Traefik is running ($TRAEFIK_READY pods)"
              kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik
            else
              echo "‚ö†Ô∏è  Traefik pods not running yet"
              kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik
            fi
          # Check for standalone Traefik namespace installation
          elif kubectl get namespace traefik 2>/dev/null; then
            echo "‚úÖ Traefik namespace exists (standalone installation)"
            kubectl get pods -n traefik
            
            TRAEFIK_READY=$(kubectl get pods -n traefik --no-headers | grep Running | wc -l)
            if [ "$TRAEFIK_READY" -ge 1 ]; then
              echo "‚úÖ Traefik is running ($TRAEFIK_READY pods)"
            fi
          else
            echo "‚ö†Ô∏è  Traefik not found (check if K3s was installed with --disable traefik)"
          fi

      # ============================================
      # Verify 7: Network connectivity
      # ============================================
      - name: Verify - Network ports are open
        run: |
          CONTROL_IP=${{ steps.tf_output.outputs.control_ip }}
          KAFKA_IP=${{ steps.tf_output.outputs.kafka_ip }}
          
          echo "Testing network connectivity..."
          
          # Test K3s API port
          if timeout 5 bash -c "cat < /dev/null > /dev/tcp/$CONTROL_IP/6443" 2>/dev/null; then
            echo "‚úÖ K3s API port 6443 is open on $CONTROL_IP"
          else
            echo "‚ùå K3s API port 6443 is not accessible"
            exit 1
          fi
          
          # Test SSH port
          if timeout 5 bash -c "cat < /dev/null > /dev/tcp/$CONTROL_IP/22" 2>/dev/null; then
            echo "‚úÖ SSH port 22 is open on $CONTROL_IP"
          else
            echo "‚ùå SSH port 22 is not accessible"
            exit 1
          fi
          
          echo "‚úÖ All critical ports are accessible"

      # ============================================
      # SERVICES INSTALLATION SECTION
      # ============================================
      # After infrastructure is ready, install and verify application services
      # Services: Strimzi Kafka Operator, (future: Prometheus, Grafana)
      
      # ============================================
      # Verify 8: Strimzi Kafka Operator is installed
      # ============================================
      - name: Verify - Strimzi Kafka Operator is installed
        run: |
          export KUBECONFIG=$PWD/kubeconfig.yaml
          
          echo "Checking Strimzi operator namespace..."
          if kubectl get namespace kafka 2>/dev/null; then
            echo "‚úÖ Kafka namespace exists"
          else
            echo "‚ùå Kafka namespace not found"
            exit 1
          fi
          
          echo ""
          echo "Checking Strimzi operator deployment..."
          if kubectl get deployment strimzi-cluster-operator -n kafka 2>/dev/null; then
            echo "‚úÖ Strimzi operator deployment exists"
          else
            echo "‚ùå Strimzi operator deployment not found"
            exit 1
          fi
          
          echo ""
          echo "Waiting for Strimzi operator to be ready..."
          if ! kubectl wait --for=condition=available deployment/strimzi-cluster-operator -n kafka --timeout=300s; then
            echo "‚ùå Strimzi operator not ready after 5 minutes"
            echo ""
            echo "Operator pod status:"
            kubectl get pods -n kafka
            echo ""
            echo "Operator logs:"
            kubectl logs -n kafka deployment/strimzi-cluster-operator --tail=50
            exit 1
          fi
          
          echo ""
          echo "Strimzi operator pod status:"
          kubectl get pods -n kafka
          
          STRIMZI_READY=$(kubectl get pods -n kafka -l name=strimzi-cluster-operator --no-headers | grep Running | wc -l)
          if [ "$STRIMZI_READY" -ge 1 ]; then
            echo "‚úÖ Strimzi operator is running ($STRIMZI_READY pods)"
          else
            echo "‚ùå Strimzi operator not running"
            kubectl get pods -n kafka
            exit 1
          fi

      # ============================================
      # Verify 9: Strimzi CRDs are installed
      # ============================================
      - name: Verify - Strimzi Kafka CRDs are available
        run: |
          export KUBECONFIG=$PWD/kubeconfig.yaml
          
          echo "Checking Strimzi Custom Resource Definitions..."
          
          REQUIRED_CRDS=(
            "kafkas.kafka.strimzi.io"
            "kafkatopics.kafka.strimzi.io"
            "kafkausers.kafka.strimzi.io"
            "kafkaconnects.kafka.strimzi.io"
            "kafkamirrormaker2s.kafka.strimzi.io"
            "kafkanodepools.kafka.strimzi.io"
          )
          
          ALL_CRDS_PRESENT=true
          for CRD in "${REQUIRED_CRDS[@]}"; do
            if kubectl get crd "$CRD" 2>/dev/null; then
              echo "‚úÖ CRD exists: $CRD"
            else
              echo "‚ùå CRD missing: $CRD"
              ALL_CRDS_PRESENT=false
            fi
          done
          
          if [ "$ALL_CRDS_PRESENT" = "false" ]; then
            echo ""
            echo "‚ùå Some Strimzi CRDs are missing"
            echo "Available Kafka-related CRDs:"
            kubectl get crd | grep kafka
            exit 1
          fi
          
          echo ""
          echo "‚úÖ All required Strimzi CRDs are installed"
          echo ""
          echo "Strimzi version information:"
          kubectl get deployment strimzi-cluster-operator -n kafka -o jsonpath='{.spec.template.spec.containers[0].image}'
          echo ""

      # ============================================
      # Verify 10: Helm is installed on control plane
      # ============================================
      - name: Verify - Helm 3 is available
        run: |
          CONTROL_IP=${{ steps.tf_output.outputs.control_ip }}
          
          echo "Checking Helm installation on control plane..."
          HELM_VERSION=$(ssh -o StrictHostKeyChecking=no root@$CONTROL_IP "helm version --short" 2>/dev/null)
          
          if [ -n "$HELM_VERSION" ]; then
            echo "‚úÖ Helm installed: $HELM_VERSION"
          else
            echo "‚ùå Helm not found on control plane"
            exit 1
          fi
          
          echo ""
          echo "Helm repositories:"
          ssh -o StrictHostKeyChecking=no root@$CONTROL_IP "helm repo list" 2>/dev/null || echo "No repositories configured"

      # ============================================
      # Upload kubeconfig artifact
      # ============================================
      - name: Upload kubeconfig artifact
        uses: actions/upload-artifact@v4
        with:
          name: kubeconfig-${{ inputs.environment }}
          path: kubeconfig.yaml
          retention-days: 7

      # ============================================
      # Bootstrap ArgoCD (Connect to Config Repo)
      # ============================================
      - name: Bootstrap ArgoCD - Connect to config repository
        # Only bootstrap if TOKEN_GIT_REPO_CONFIG is configured
        # Skip for initial deployments before config repo exists
        continue-on-error: true  # Don't fail deployment if config repo not ready yet
        run: |
          export KUBECONFIG=$PWD/kubeconfig.yaml
          
          echo "========================================"
          echo "  ArgoCD Bootstrap - Auto Configuration"
          echo "========================================"
          echo ""
          echo "‚öôÔ∏è  Configuring ArgoCD to access trading-cz/config repository..."
          
          # Test if config repo exists and is accessible
          if curl -s -o /dev/null -w "%{http_code}" -H "Authorization: token ${{ secrets.TOKEN_GIT_REPO_CONFIG }}" \
             https://api.github.com/repos/trading-cz/config | grep -q "200"; then
            echo "‚úÖ Config repository is accessible"
          else
            echo "‚ö†Ô∏è  Config repository not found or not accessible"
            echo "   Create trading-cz/config repository first, then re-run deployment"
            echo "   Or bootstrap manually using modules/argocd-bootstrap/"
            exit 0  # Exit gracefully
          fi
          
          # Create repository secret using TOKEN_GIT_REPO_CONFIG
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Secret
          metadata:
            name: trading-config-repo
            namespace: argocd
            labels:
              argocd.argoproj.io/secret-type: repository
          type: Opaque
          stringData:
            type: git
            url: https://github.com/trading-cz/config.git
            password: ${{ secrets.TOKEN_GIT_REPO_CONFIG }}
            username: not-used
          EOF
          
          echo "‚úÖ Repository secret created"
          echo ""
          
          # Deploy parent application
          kubectl apply -f modules/argocd-bootstrap/parent-app.yaml
          echo "‚úÖ Parent application deployed"
          echo ""
          
          # Deploy ArgoCD ingress for external access
          echo "‚öôÔ∏è  Deploying ArgoCD ingress..."
          kubectl apply -f modules/argocd-bootstrap/argocd-ingress.yaml
          echo "‚úÖ ArgoCD ingress deployed (accessible via https://<control-plane-ip>)"
          echo ""
          
          # Set custom admin password if provided
          if [ -n "${{ secrets.ARGOCD_ADMIN_PASSWORD }}" ]; then
            echo "üîê Setting custom admin password..."
            
            # Install bcrypt for Python (needed for password hashing)
            python3 -m pip install bcrypt --quiet
            
            # Generate bcrypt hash
            BCRYPT_HASH=$(python3 -c "import bcrypt; print(bcrypt.hashpw('${{ secrets.ARGOCD_ADMIN_PASSWORD }}'.encode('utf-8'), bcrypt.gensalt(rounds=10)).decode('utf-8'))")
            
            # Delete initial secret
            kubectl delete secret argocd-initial-admin-secret -n argocd 2>/dev/null || true
            
            # Update admin password
            kubectl patch secret argocd-secret -n argocd --type merge \
              -p "{\"stringData\": {\"admin.password\": \"$BCRYPT_HASH\", \"admin.passwordMtime\": \"$(date +%FT%T%Z)\"}}"
            
            echo "‚úÖ Custom admin password configured"
            echo "   Username: admin"
            echo "   Password: (from ARGOCD_ADMIN_PASSWORD secret)"
          else
            echo "‚ÑπÔ∏è  Using default admin password (get from argocd-initial-admin-secret)"
          fi
          echo ""
          
          # Wait for applications to be created
          echo "‚è≥ Waiting for ArgoCD to detect applications..."
          sleep 15
          
          echo "üìã ArgoCD Applications:"
          kubectl get applications -n argocd || echo "No applications found yet (config repo may be empty)"
          echo ""
          echo "‚úÖ ArgoCD bootstrap complete!"
          echo "   ArgoCD will now auto-sync applications from trading-cz/config"
          echo ""
          echo "Next: Push Kubernetes manifests to trading-cz/config repo"

      # ============================================
      # Verify 11: ArgoCD is fully operational
      # ============================================
      - name: Verify - ArgoCD server is operational
        if: success()  # Only run if bootstrap succeeded
        run: |
          export KUBECONFIG=$PWD/kubeconfig.yaml
          
          echo "========================================"
          echo "  ArgoCD Operational Verification"
          echo "========================================"
          echo ""
          
          # Check ArgoCD server health endpoint
          echo "1Ô∏è‚É£  Checking ArgoCD server health..."
          
          # Port-forward in background (use job control to kill later)
          kubectl port-forward svc/argocd-server -n argocd 8080:443 > /dev/null 2>&1 &
          PF_PID=$!
          sleep 5
          
          # Test health endpoint
          if curl -k -s https://localhost:8080/healthz | grep -q "ok"; then
            echo "‚úÖ ArgoCD server health endpoint responding"
          else
            echo "‚ö†Ô∏è  ArgoCD server health endpoint not responding (may still be initializing)"
          fi
          
          # Kill port-forward
          kill $PF_PID 2>/dev/null || true
          echo ""
          
          # Check repository connection
          echo "2Ô∏è‚É£  Checking repository connections..."
          kubectl get secrets -n argocd -l argocd.argoproj.io/secret-type=repository
          
          REPO_COUNT=$(kubectl get secrets -n argocd -l argocd.argoproj.io/secret-type=repository --no-headers | wc -l)
          if [ "$REPO_COUNT" -ge 1 ]; then
            echo "‚úÖ Repository credentials configured ($REPO_COUNT repos)"
          else
            echo "‚ö†Ô∏è  No repository credentials found"
          fi
          echo ""
          
          # Check applications
          echo "3Ô∏è‚É£  Checking ArgoCD applications..."
          kubectl get applications -n argocd -o wide || echo "No applications deployed yet"
          
          APP_COUNT=$(kubectl get applications -n argocd --no-headers 2>/dev/null | wc -l)
          if [ "$APP_COUNT" -ge 1 ]; then
            echo "‚úÖ ArgoCD applications found: $APP_COUNT"
            echo ""
            echo "Application statuses:"
            kubectl get applications -n argocd -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.health.status}{"\t"}{.status.sync.status}{"\n"}{end}' 2>/dev/null | \
              awk 'BEGIN{print "NAME\tHEALTH\tSYNC"} {print}' || echo "Unable to fetch status"
          else
            echo "‚ö†Ô∏è  No applications found (config repo may be empty or not synced yet)"
          fi
          echo ""
          
          # Check admin credentials and ingress
          echo "4Ô∏è‚É£  Checking ArgoCD access..."
          
          CONTROL_IP=${{ steps.tf_output.outputs.control_ip }}
          
          # Check ingress
          if kubectl get ingress argocd-server-ingress -n argocd > /dev/null 2>&1; then
            echo "‚úÖ ArgoCD ingress configured"
            echo ""
            echo "üìã Access ArgoCD UI:"
            echo "   URL: https://$CONTROL_IP"
            echo "   Username: admin"
            
            if [ -n "${{ secrets.ARGOCD_ADMIN_PASSWORD }}" ]; then
              echo "   Password: (from ARGOCD_ADMIN_PASSWORD secret)"
            else
              echo "   Password: kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d"
            fi
            
            echo ""
            echo "‚ö†Ô∏è  Accept self-signed certificate warning in browser"
          else
            echo "‚ö†Ô∏è  ArgoCD ingress not found"
            echo "   Use port-forward: kubectl port-forward svc/argocd-server -n argocd 8080:443"
          fi
          echo ""
          
          echo "========================================"
          echo "‚úÖ ArgoCD Verification Complete"
          echo "========================================"

      # ============================================
      # Summary
      # ============================================
      - name: Deployment summary
        if: always()
        run: |
          echo "========================================"
          echo "  Deployment completed for ${{ inputs.environment }}"
          echo "========================================"
          echo ""
          echo "‚úÖ Infrastructure provisioned via Terraform"
          echo "‚úÖ K3s cluster operational (2 nodes)"
          echo "‚úÖ ArgoCD installed and running"
          echo "‚úÖ Traefik ingress ready"
          echo "‚úÖ Helm 3 installed"
          echo "‚úÖ Strimzi Kafka Operator 0.48.0 installed"
          echo "‚úÖ Kafka CRDs available (KRaft mode ready)"
          echo "‚úÖ ArgoCD configured (connected to trading-cz/config repo)"
          echo "‚úÖ ArgoCD ingress exposed via Traefik"
          echo ""
          echo "üåê Access Information:"
          echo "  Control Plane IP: ${{ steps.tf_output.outputs.control_ip }}"
          echo "  ArgoCD UI: https://${{ steps.tf_output.outputs.control_ip }}"
          echo "  Username: admin"
          if [ -n "${{ secrets.ARGOCD_ADMIN_PASSWORD }}" ]; then
            echo "  Password: (stored in ARGOCD_ADMIN_PASSWORD secret)"
          else
            echo "  Password: Get via 'kubectl -n argocd get secret argocd-initial-admin-secret'"
          fi
          echo ""
          echo "Next steps:"
          echo "  1. Access ArgoCD UI and verify applications are syncing"
          echo "  2. Push changes to trading-cz/config repo to deploy new apps"
          echo "  3. Monitor deployments in ArgoCD UI"
          echo ""
          echo "Future services (not yet installed):"
          echo "  - Prometheus (metrics collection)"
          echo "  - Grafana (visualization)"
          echo ""
          echo "To destroy cluster: Use 'hcloud-maintenance' workflow with 'destroy-cluster' action"
          echo "========================================"